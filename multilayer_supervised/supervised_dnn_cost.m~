function [ cost, grad, pred_prob] = supervised_dnn_cost( theta, ei, data, labels, pred_only)
%SPNETCOSTSLAVE Slave cost function for simple phone net
%   Does all the work of cost / gradient computation
%   Returns cost broken into cross-entropy, weight norm, and prox reg
%        components (ceCost, wCost, pCost)

%% default values
po = false;
if exist('pred_only','var')
  po = pred_only;
end;

%% reshape into network
stack = params2stack(theta, ei);
numHidden = numel(ei.layer_sizes) - 1;
hAct = cell(numHidden+1, 1);
gradStack = cell(numHidden+1, 1);
%% forward prop
%%% YOUR CODE HERE %%%
a = cell(numHidden + 2, 1);
a{1} = data;
z = cell(numHidden + 2, 1);
delta = cell(numHidden + 2, 1);
m = size(data, 2);
for l = 1 : numHidden
    a{l + 1} = zeros(size(stack{l}.b, 1), 1);
    delta{l + 1} = zeros(size(stack{l}.b, 1), 1);
   
    z{l + 1} = zeros(size(stack{l}.b, 1), 1);
    if l == 1
        b = repmat(stack{l}.b, 1, size(data, 2));
        z{l + 1} = stack{l}.W * data;
    else
        b = repmat(stack{l}.b, 1, size(a{l}, 2));
        z{l + 1} = stack{l}.W * a{l};
    end
    z{l + 1} = z{l + 1} + b;
    a{l + 1} = 1 ./ (1 + exp(-1 * z{l + 1}));
end

temp = stack{numHidden + 1}.W * a{numHidden + 1};
b = repmat(stack{numHidden + 1}.b, 1, size(a{numHidden + 1}, 2));
%temp = exp(temp);
temp = temp + b;
temp = exp(temp);
sumtemp = sum(temp, 1);
sumtemp = repmat(sumtemp, size(temp, 1), 1);
pred_prob = temp ./ sumtemp;

%% return here if only predictions desired.
if po
  cost = -1; ceCost = -1; wCost = -1; numCorrect = -1;
  grad = [];  
  return;
end;

%% compute cost
%%% YOUR CODE HERE %%%
I = sub2ind(size(pred_prob), labels', 1 : size(pred_prob, 2));
f = log(pred_prob(I));
a{numHidden + 2} = pred_prob;
sigmaW = 0;
for i = 1 : numel(stack)
    temp = stack{i}.W .^ 2;
    sigmaW = sigmaW + sum(temp(:));
end

ceCost = -1 * sum(f(:)) ./ m;
wCost = 0;
%cost = -1 * sum(f(:))  + lambda / 2 * sigmaW;
%cost = cost ./ m;
%% compute gradients using backpropagation
%%% YOUR CODE HERE %%%
w = (1 : size(pred_prob, 1))';
w = repmat(w, 1, size(pred_prob, 2));
global y;
y = repmat(labels', size(w, 1), 1);
w = (w == y);
w = pred_prob - w;
delta{numHidden + 2} = w;
%end
%% compute weight penalty cost and gradient for non-bias terms
%%% YOUR CODE HERE %%%
%delta{numHidden + 2} = repmat(delta{numHidden + 2},1, size(a{numHidden + 2}, 2)) ./ m;
for i = 1 : numHidden
    l = numHidden - i + 2;
    %delta{l} = stack{l}.W' * delta{l + 1};
	delta{l} = stack{l}.W' * delta{l + 1};
    d = a{l} .* (1 - a{l});
    %delta{l} = repmat(delta{l}, 1, size(a{l}, 2));
    delta{l} = delta{l} .* d;
end
for l = 1 : numHidden + 1
    %if l == 1
    %    gradStack{l}.W = delta{l + 1} * data';
    %else
    %    gradStack{l}.W = delta{l + 1} * a{l}';
    %end
    gradStack{l}.W = delta{l + 1} * a{l}' ./ m;
    gradStack{l}.W = gradStack{l}.W + stack{l}.W * ei.lambda;
    %gradStack{l}.W = zeros(size(delta{l + 1}, 1), size(a{l}, 1));
    %fprintf('2\n');
    %for i = 1 : m
    %    gradStack{l}.W = gradStack{l}.W + delta{l + 1}(:, i) * a{l}(:, i)';
    %end
    %gradStack{l}.W = gradStack{l}.W;
    %gradStack{l}.W = sum(gradStack{l}.W, 2);
    gradStack{l}.b = delta{l + 1};
    %gradStack{l}.b = gradStack{l}.b ./ m;
    gradStack{l}.b = sum(gradStack{l}.b, 2) ./ m;
    %gradStack{l}.b = sum(gradStack{l}.b, 2);
    %fprintf('size of gradStack{l}.W = %d\n', size(gradStack{l}.W));
    %fprintf('size of gradStack{l}.b = %d\n', size(gradStack{l}.b));
end


%% reshape gradients into vector
[grad] = stack2params(gradStack);
cost = ceCost + wCost;
end
